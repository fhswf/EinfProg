{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1c8a7d",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Einführung in die Programmierung\n",
    "### Winterersemester 2025/25\n",
    "Prof. Dr. Stefan Goetze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb9937-2dff-4211-bea3-1d3f654dce58",
   "metadata": {},
   "source": [
    "# Übung X - Aufgabe 1 - Klassifikation von Zahlen mit einem CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7951e2a9-0646-443b-9416-b6547ec33de1",
   "metadata": {},
   "source": [
    "## 1. Nötige Imports vornehmen\n",
    "\n",
    "Die folgende Code-Zellen überprüfen zuerst, ob PyTorch installiert ist und importieren dann die notwendigen Bibliotheken, die im Folgenden benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if PyTorch is already installed\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch is not installed. Installing now...\")\n",
    "    %conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26014ac4-85d7-4976-b570-694eede0e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "#import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "#import numpy as np\n",
    "#import pickle  # oder alternativ json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed80cf-4d9f-49b6-937e-f932a94edddc",
   "metadata": {},
   "source": [
    "## 2. Trainingsdaten laden\n",
    "\n",
    "Zunächst werden die Trainingsdaten geladen. Dabei wollen wir den [MNIST-Zahlen Datenset][MNIST] verwenden. Die Funktion *[toTensor()][PyTorchToTensor]* skaliert die Bilder automatisch nach `[0.0, 1.0]`. Daher ist eine manuelle Skalierung nicht nötig.\n",
    "\n",
    "Die Skalierung ist wichtig, weil neuronale Netzwerke empfindlich auf den Wertebereich der Eingabedaten reagieren. Eingabewerte im Bereich `[0.0, 1.0]` sorgen für eine stabilere und schnellere Konvergenz beim Training. Würden wir z. B. die rohen Pixelwerte von 0 bis 255 verwenden, könnten Gradienten zu groß oder zu klein werden, was das Training erschwert oder verlangsamt.\n",
    "\n",
    "[PyTorchToTensor]: https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor\n",
    "[MNIST]: https://de.wikipedia.org/wiki/MNIST-Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e93a5-fd47-4cae-b715-ff53e74375dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdd797-46fa-4762-bb24-e7da69de88cd",
   "metadata": {},
   "source": [
    "Schaun wir uns an, aus wie vielen Datenpunkte unser Datensatz besteht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4f8ab-2c3a-44ab-b5a0-d7b3a8d0cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e56b9-9f5c-46cd-9385-dbb82e905b32",
   "metadata": {},
   "source": [
    "Wir können uns auch anschauen, im welchen Format die Daten vorliegen. Dies wird später für das CNN wichtig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e14bb6-e48e-4df2-904f-64af74098f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9ed81-5323-4162-966e-b5f41fc52c0b",
   "metadata": {},
   "source": [
    "### 2.1 Anzeigen der Bilder aus dem Datensatz\n",
    "\n",
    "Statt `inp[0]` kann auch die Funktion *[squeeze()][2]* verwendet werden. Wie wir vorher gesehen haben, haben die Bilddaten die Form `[1, 28, 28]`, wobei die erste Dimension den Kanal (in diesem Fall Graustufen) repräsentiert.\n",
    "\n",
    "Da `plt.imshow()` jedoch ein 2D-Array erwartet (Form `[H, W]`), muss die Kanaldimension entfernt werden. Sowohl `inp[0]` als auch `torch.squeeze(inp)` liefern das gewünschte Format `[28, 28]`.\n",
    "\n",
    "[2]: https://pytorch.org/docs/main/generated/torch.squeeze.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef85e88-cfdb-462a-be68-202b30b240e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp):\n",
    "    plt.imshow(inp[0], cmap='gray')  # Graustufenbild anzeigen\n",
    "    plt.axis('off')  # Achsen ausblenden\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83036760-541b-48fd-922e-b60040de65e8",
   "metadata": {},
   "source": [
    "Die folgende Zelle zeigen 50 Beispielbilder aus dem Testdatensatz, indem aus jedem Bild der erste (und einzige) Kanal `image[0]` extrahiert und als Graustufenbild mit dem zugehörigen Label dargestellt wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3c9eb-593b-4481-a9c3-a780c52b7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 10, figsize=(12, 7), subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "# Zeige einige Bilder aus dem Trainingsdatensatz an\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = valid_set[i]\n",
    "    ax.imshow(image[0], cmap=plt.cm.gray_r)  \n",
    "    ax.set_title('label ' +str(i) + ': ' + str(label), fontsize=8)\n",
    "\n",
    "# add title\n",
    "plt.suptitle('MNIST Dataset Samples', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21f89c-e6a8-4603-9b53-6088519d97ec",
   "metadata": {},
   "source": [
    "Beispielbild aus dem Trainingsdatensatz anzeigen mit Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba26839-ea84-4d3e-9f59-90b7e36a88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(train_set[500][0])\n",
    "print(f\"Label: {valid_set[500][1]}\")  # Das zugehörige Label anzeigen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc88782a-9795-4144-ae62-12cb54fd7c93",
   "metadata": {},
   "source": [
    "## 3. Convolutional Neural Network erstellen\n",
    "\n",
    "Das CNN basiert auf der Eingabegröße `[1, 28, 28]` der MNIST-Bilder. Die erste Convolution-Schicht verwendet einen 3×3-Kernel mit Padding=1. Padding sorgt dafür, dass die räumliche Auflösung nach der Faltung erhalten bleibt (28×28).  \n",
    "\n",
    "Nach jedem Convolution-Schritt folgt ein MaxPooling mit einem 2×2-Fenster, das Höhe und Breite halbiert.\n",
    "\n",
    "Die Form der Feature Maps verändert sich dabei wie folgt:\n",
    "- Nach der ersten Conv- und Pooling-Schicht: `(32, 14, 14)`\n",
    "- Nach der zweiten Conv- und Pooling-Schicht: `(64, 7, 7)`\n",
    "\n",
    "Bevor die Daten in die Fully Connected Layer gehen, werden sie flach gemacht. Deshalb muss die Eingabegröße für `fc1` genau `64 * 7 * 7` sein, also 3136.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7901cae-c7c4-48b8-b5a8-82cba8688929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)   # (1, 28, 28) -> (32, 28, 28)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # (32, 14, 14) -> (64, 14, 14)\n",
    "        self.pool = nn.MaxPool2d(2, 2)                            # halbiert Auflösung\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 Klassen für Ziffern 0–9\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # (32, 28, 28) -> (32, 14, 14)\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # (64, 14, 14) -> (64, 7, 7)\n",
    "        x = x.view(-1, 64 * 7 * 7)             # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b83d1-1b9c-4d56-8225-44afab3568e1",
   "metadata": {},
   "source": [
    "## 4. Trainieren des CNNs\n",
    "\n",
    "Wir trainieren unser CNN-Modell und visualisieren anschließend die Ergebnisse. Wie wir sehen, brauchen wir tatsächlich nicht viele Epochen – bereits nach **3 Epochen** erreichen wir eine hohe Genauigkeit.\n",
    "\n",
    "Das liegt daran, dass der **MNIST-Datensatz** relativ einfach ist:  \n",
    "Er besteht aus kleinen, zentrierten Graustufenbildern von handgeschriebenen Ziffern (28x28 Pixel), die sich gut voneinander unterscheiden. Ein CNN kann durch seine Fähigkeit zur automatischen Merkmalsextraktion (z. B. durch Faltung und Pooling) schnell lernen, worauf es bei den Ziffern ankommt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffbb5a5-6da3-43fc-888d-c823025ba792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(epochs, training_acc, testing_acc, training_loss, testing_loss):\n",
    "    plt.plot(range(epochs), training_acc, label=\"train_acc\")\n",
    "    plt.plot(range(epochs), testing_acc, label=\"valid_acc\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(epochs), training_loss, label=\"train_loss\")\n",
    "    plt.plot(range(epochs), testing_loss, label=\"valid_loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, epochs=3, learning_rate=0.001):\n",
    "    # Hier Initialisierung der Verlustfunktion und des Optimierers\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_loss = []\n",
    "    testing_loss = []\n",
    "    training_acc = []\n",
    "    testing_acc = []\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "    \n",
    "    # Training des Modells\n",
    "    with tqdm(range(epochs)) as iterator:\n",
    "        for epoch in iterator:\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "\n",
    "            model.train()\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(images)\n",
    "                loss = loss_fn(output, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                prediction = torch.argmax(output, dim=1)\n",
    "                train_acc += (prediction == labels).sum().item()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            training_acc.append(train_acc/len(train_set))\n",
    "            training_loss.append(train_loss/len(train_set))\n",
    "\n",
    "            # Evaluation des Modells auf den Testdaten\n",
    "            test_loss = 0\n",
    "            test_acc = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in valid_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    output = model(images)\n",
    "                    loss = loss_fn(output, labels)\n",
    "                    prediction = torch.argmax(output, dim=1)\n",
    "\n",
    "                    test_acc += (prediction == labels).sum().item()\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                testing_acc.append(test_acc/len(valid_set))\n",
    "                testing_loss.append(test_loss/len(valid_set))\n",
    "\n",
    "            iterator.set_postfix_str(f\"train_acc: {train_acc/len(train_set):.2f} test_acc: {test_acc/len(valid_set):.2f} train_loss: {train_loss/len(train_set):.2f} test_loss: {test_loss/len(valid_set):.2f}\")\n",
    "\n",
    "    # Modell speichern\n",
    "    torch.save(model.state_dict(), 'model_weights.pth')\n",
    "    \n",
    "    # Speichere die Metriken in einer Datei\n",
    "    metrics = {\n",
    "        'training_acc': training_acc,\n",
    "        'testing_acc': testing_acc,\n",
    "        'training_loss': training_loss,\n",
    "        'testing_loss': testing_loss\n",
    "    }\n",
    "    \n",
    "    #with open('training_metrics.pkl', 'wb') as f:\n",
    "     #   pickle.dump(metrics, f)\n",
    "\n",
    "    plot_results(epochs, training_acc, testing_acc, training_loss, testing_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072e064-3951-4ad3-8e96-fe111ca9a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79f84f-d361-4ce3-98bd-e8b0fa82f5c3",
   "metadata": {},
   "source": [
    "Wir können raus lesen, dass unser Modell am Ende eine Accuracy von 99,2% hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231240db-754d-43dd-a06e-60acf8cf367c",
   "metadata": {},
   "source": [
    "Schauen wir uns ein Test image an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38a159-962d-47ec-ac93-a1aceef2f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell in Evaluationsmodus\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Ein Bild aus dem Testdatensatz holen\n",
    "image, label = valid_set[22]  # z.B. zufälliges Bild\n",
    "image = image.unsqueeze(0).to(device)  # Batch-Dimension hinzufügen: (1, 1, 28, 28)\n",
    "\n",
    "# Vorhersage machen\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    pred = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Bild anzeigen mit Prediction\n",
    "plt.imshow(image.cpu().squeeze(), cmap='gray')\n",
    "plt.title(f\"Prediction: {pred} | Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
